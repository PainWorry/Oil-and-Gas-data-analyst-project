{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark ETL: Pipeline & Logistics Data Integration\n",
    "\n",
    "## Objective\n",
    "Load, join, and cleanse data from PostgreSQL (Inspection Logs) and MongoDB (Asset Telemetry). \n",
    "Aggregate sensor data to match inspection windows and persist as Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# Set environment variables for PySpark to use the correct Python executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "# Set HADOOP_HOME to the local hadoop directory\n",
    "os.environ['HADOOP_HOME'] = os.path.abspath('../hadoop')\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "\n",
    "# Set environment variables for PySpark to use the correct Python executable\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, avg, max, min\n",
    "\n",
    "# Initialize Spark Session\n",
    "# Note: In a real environment, you would need to include the necessary JARs for Postgres and MongoDB connectors\n",
    "# .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.postgresql:postgresql:42.2.18\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PTT_Pipeline_Logistics_ETL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------+------------+--------------+---------------+--------------------+\n",
      "|inspection_id|pipeline_segment_id|inspection_date|inspector_id|crack_detected|corrosion_level|maintenance_required|\n",
      "+-------------+-------------------+---------------+------------+--------------+---------------+--------------------+\n",
      "|            1|            SEG-101|     2023-10-01|      INS-01|         false|            0.0|               false|\n",
      "|            2|            SEG-102|     2023-10-02|      INS-02|          true|           15.5|                true|\n",
      "|            3|            SEG-103|     2023-10-03|      INS-01|         false|            2.1|               false|\n",
      "+-------------+-------------------+---------------+------------+--------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data from PostgreSQL (Mocked for this notebook as we don't have a live DB)\n",
    "# In production: spark.read.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://localhost:5432/ptt_db\")...\n",
    "\n",
    "inspection_data = [\n",
    "    (1, \"SEG-101\", \"2023-10-01\", \"INS-01\", False, 0.0, False),\n",
    "    (2, \"SEG-102\", \"2023-10-02\", \"INS-02\", True, 15.5, True),\n",
    "    (3, \"SEG-103\", \"2023-10-03\", \"INS-01\", False, 2.1, False)\n",
    "]\n",
    "columns = [\"inspection_id\", \"pipeline_segment_id\", \"inspection_date\", \"inspector_id\", \"crack_detected\", \"corrosion_level\", \"maintenance_required\"]\n",
    "\n",
    "df_inspections = spark.createDataFrame(inspection_data, columns)\n",
    "df_inspections.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+-------------+---------------+\n",
      "|pipeline_segment_id|          timestamp|pressure_psi|temperature_c|vibration_level|\n",
      "+-------------------+-------------------+------------+-------------+---------------+\n",
      "|            SEG-101|2023-10-01 10:00:00|      1000.0|         30.0|            0.5|\n",
      "|            SEG-101|2023-10-01 11:00:00|      1050.0|         32.0|            0.6|\n",
      "|            SEG-102|2023-10-02 09:00:00|       900.0|         28.0|            2.5|\n",
      "|            SEG-102|2023-10-02 10:00:00|       850.0|         29.0|            3.0|\n",
      "+-------------------+-------------------+------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Data from MongoDB (Mocked)\n",
    "# In production: spark.read.format(\"mongo\").option(\"uri\", \"mongodb://localhost:27017/ptt_logistics_db.asset_telemetry\").load()\n",
    "\n",
    "telemetry_data = [\n",
    "    (\"SEG-101\", \"2023-10-01 10:00:00\", 1000.0, 30.0, 0.5),\n",
    "    (\"SEG-101\", \"2023-10-01 11:00:00\", 1050.0, 32.0, 0.6),\n",
    "    (\"SEG-102\", \"2023-10-02 09:00:00\", 900.0, 28.0, 2.5),\n",
    "    (\"SEG-102\", \"2023-10-02 10:00:00\", 850.0, 29.0, 3.0)\n",
    "]\n",
    "telemetry_cols = [\"pipeline_segment_id\", \"timestamp\", \"pressure_psi\", \"temperature_c\", \"vibration_level\"]\n",
    "\n",
    "df_telemetry = spark.createDataFrame(telemetry_data, telemetry_cols)\n",
    "df_telemetry = df_telemetry.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "df_telemetry.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+---------------+------------+--------------+---------------+--------------------+------------+--------+-------------+\n",
      "|pipeline_segment_id|inspection_id|inspection_date|inspector_id|crack_detected|corrosion_level|maintenance_required|avg_pressure|avg_temp|max_vibration|\n",
      "+-------------------+-------------+---------------+------------+--------------+---------------+--------------------+------------+--------+-------------+\n",
      "|            SEG-101|            1|     2023-10-01|      INS-01|         false|            0.0|               false|      1025.0|    31.0|          0.6|\n",
      "|            SEG-102|            2|     2023-10-02|      INS-02|          true|           15.5|                true|       875.0|    28.5|          3.0|\n",
      "|            SEG-103|            3|     2023-10-03|      INS-01|         false|            2.1|               false|        NULL|    NULL|         NULL|\n",
      "+-------------------+-------------+---------------+------------+--------------+---------------+--------------------+------------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Aggregation & Join\n",
    "# Aggregate telemetry by segment and day (simplified)\n",
    "\n",
    "df_telemetry_agg = df_telemetry.groupBy(\"pipeline_segment_id\") \\\n",
    "    .agg(\n",
    "        avg(\"pressure_psi\").alias(\"avg_pressure\"),\n",
    "        avg(\"temperature_c\").alias(\"avg_temp\"),\n",
    "        max(\"vibration_level\").alias(\"max_vibration\")\n",
    "    )\n",
    "\n",
    "# Join with Inspections\n",
    "df_final = df_inspections.join(df_telemetry_agg, \"pipeline_segment_id\", \"left\")\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../data/processed_pipeline_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# 4. Persist to Parquet\n",
    "output_path = \"../data/processed_pipeline_data.parquet\"\n",
    "df_final.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
