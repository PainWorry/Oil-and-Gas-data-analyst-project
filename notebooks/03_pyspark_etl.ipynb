{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark ETL: Pipeline & Logistics Data Integration\n",
    "\n",
    "## Objective\n",
    "Load, join, and cleanse data from PostgreSQL (Inspection Logs) and MongoDB (Asset Telemetry). \n",
    "Aggregate sensor data to match inspection windows and persist as Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# Set environment variables for PySpark to use the correct Python executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "# Set HADOOP_HOME to the local hadoop directory\n",
    "os.environ['HADOOP_HOME'] = os.path.abspath('../hadoop')\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "\n",
    "# Set environment variables for PySpark to use the correct Python executable\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, avg, max, min\n",
    "\n",
    "# Initialize Spark Session\n",
    "# Note: In a real environment, you would need to include the necessary JARs for Postgres and MongoDB connectors\n",
    "# .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.postgresql:postgresql:42.2.18\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PTT_Pipeline_Logistics_ETL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------+------------+--------------+---------------+--------------------+\n",
      "|inspection_id|pipeline_segment_id|inspection_date|inspector_id|crack_detected|corrosion_level|maintenance_required|\n",
      "+-------------+-------------------+---------------+------------+--------------+---------------+--------------------+\n",
      "|            1|            SEG-101|     2023-10-14|      INS-05|          true|           19.6|                true|\n",
      "|            2|            SEG-102|     2023-10-09|      INS-04|         false|           19.1|                true|\n",
      "|            3|            SEG-103|     2023-10-17|      INS-04|          true|           11.1|                true|\n",
      "|            4|            SEG-104|     2023-10-17|      INS-01|         false|           11.6|                true|\n",
      "|            5|            SEG-105|     2023-10-29|      INS-05|         false|           17.2|                true|\n",
      "+-------------+-------------------+---------------+------------+--------------+---------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data from PostgreSQL (Mocked)\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "inspection_data = []\n",
    "for i in range(1, 21):  # Generate 20 segments\n",
    "    seg_id = f\"SEG-{100+i}\"\n",
    "    insp_date = (datetime(2023, 10, 1) + timedelta(days=random.randint(0, 30))).strftime(\"%Y-%m-%d\")\n",
    "    inspector = f\"INS-{random.randint(1, 5):02d}\"\n",
    "    crack = random.choice([True, False])\n",
    "    corrosion = round(random.uniform(0, 20), 1)\n",
    "    maint_req = crack or (corrosion > 10)\n",
    "    inspection_data.append((i, seg_id, insp_date, inspector, crack, corrosion, maint_req))\n",
    "\n",
    "columns = [\"inspection_id\", \"pipeline_segment_id\", \"inspection_date\", \"inspector_id\", \"crack_detected\", \"corrosion_level\", \"maintenance_required\"]\n",
    "\n",
    "df_inspections = spark.createDataFrame(inspection_data, columns)\n",
    "df_inspections.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+-------------+---------------+\n",
      "|pipeline_segment_id|          timestamp|pressure_psi|temperature_c|vibration_level|\n",
      "+-------------------+-------------------+------------+-------------+---------------+\n",
      "|            SEG-101|2023-10-01 10:00:00|       815.7|         29.9|           0.53|\n",
      "|            SEG-101|2023-10-01 11:00:00|       917.9|         31.3|           1.56|\n",
      "|            SEG-101|2023-10-01 12:00:00|       989.3|         27.8|           3.89|\n",
      "|            SEG-101|2023-10-01 13:00:00|      1115.4|         27.3|           2.71|\n",
      "|            SEG-101|2023-10-01 14:00:00|      1023.1|         33.3|           1.17|\n",
      "+-------------------+-------------------+------------+-------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Data from MongoDB (Mocked)\n",
    "telemetry_data = []\n",
    "for i in range(1, 21):  # Generate telemetry for ALL 20 segments\n",
    "    seg_id = f\"SEG-{100+i}\"\n",
    "    # Generate 5 readings per segment\n",
    "    for j in range(5):\n",
    "        ts = (datetime(2023, 10, 1, 10, 0) + timedelta(hours=j)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        pressure = round(random.uniform(800, 1200), 1)\n",
    "        temp = round(random.uniform(25, 35), 1)\n",
    "        vibration = round(random.uniform(0, 5), 2)\n",
    "        telemetry_data.append((seg_id, ts, pressure, temp, vibration))\n",
    "\n",
    "telemetry_cols = [\"pipeline_segment_id\", \"timestamp\", \"pressure_psi\", \"temperature_c\", \"vibration_level\"]\n",
    "\n",
    "df_telemetry = spark.createDataFrame(telemetry_data, telemetry_cols)\n",
    "df_telemetry = df_telemetry.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "df_telemetry.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+---------------+------------+--------------+---------------+--------------------+------------------+------------------+-------------+\n",
      "|pipeline_segment_id|inspection_id|inspection_date|inspector_id|crack_detected|corrosion_level|maintenance_required|      avg_pressure|          avg_temp|max_vibration|\n",
      "+-------------------+-------------+---------------+------------+--------------+---------------+--------------------+------------------+------------------+-------------+\n",
      "|            SEG-101|            1|     2023-10-14|      INS-05|          true|           19.6|                true|            972.28|29.919999999999998|         3.89|\n",
      "|            SEG-103|            3|     2023-10-17|      INS-04|          true|           11.1|                true|1015.5600000000001|28.839999999999996|         3.65|\n",
      "|            SEG-102|            2|     2023-10-09|      INS-04|         false|           19.1|                true| 976.9799999999999|              28.6|         4.26|\n",
      "|            SEG-105|            5|     2023-10-29|      INS-05|         false|           17.2|                true|1146.6399999999999|             29.72|         3.28|\n",
      "|            SEG-104|            4|     2023-10-17|      INS-01|         false|           11.6|                true|           1035.26|31.080000000000002|         2.84|\n",
      "|            SEG-106|            6|     2023-10-12|      INS-05|         false|            1.0|               false| 994.9800000000001|             28.48|          4.7|\n",
      "|            SEG-108|            8|     2023-10-10|      INS-02|         false|            6.2|               false|            1041.7|28.860000000000003|         4.24|\n",
      "|            SEG-107|            7|     2023-10-17|      INS-05|          true|           10.4|                true|           1019.68|31.179999999999996|         4.53|\n",
      "|            SEG-110|           10|     2023-10-23|      INS-01|         false|            4.9|               false|           1028.08|              30.6|         4.68|\n",
      "|            SEG-109|            9|     2023-10-30|      INS-04|          true|           17.7|                true|            955.18|             31.98|         3.87|\n",
      "|            SEG-111|           11|     2023-10-02|      INS-03|          true|           18.0|                true| 975.3399999999999|29.160000000000004|         2.54|\n",
      "|            SEG-113|           13|     2023-10-25|      INS-04|          true|           14.5|                true|            1068.5|             29.78|         4.63|\n",
      "|            SEG-112|           12|     2023-10-24|      INS-02|          true|            3.4|                true|            1044.9|             30.72|         2.59|\n",
      "|            SEG-115|           15|     2023-10-19|      INS-03|         false|           10.5|                true|            927.86|28.639999999999997|         4.56|\n",
      "|            SEG-114|           14|     2023-10-20|      INS-03|         false|            6.3|               false| 985.7400000000001|31.259999999999998|         4.89|\n",
      "|            SEG-116|           16|     2023-10-05|      INS-04|         false|           14.7|                true|             967.0|30.660000000000004|         3.67|\n",
      "|            SEG-118|           18|     2023-10-29|      INS-01|         false|           12.6|                true|           1071.24|              27.3|         4.69|\n",
      "|            SEG-117|           17|     2023-10-22|      INS-05|         false|            4.0|               false| 996.5600000000001|              27.8|         3.01|\n",
      "|            SEG-120|           20|     2023-10-03|      INS-01|         false|           11.1|                true|1075.6399999999999|              29.0|         4.88|\n",
      "|            SEG-119|           19|     2023-10-02|      INS-03|         false|           19.4|                true|1046.5199999999998|30.560000000000002|         4.29|\n",
      "+-------------------+-------------+---------------+------------+--------------+---------------+--------------------+------------------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Aggregation & Join\n",
    "# Aggregate telemetry by segment and day (simplified)\n",
    "\n",
    "df_telemetry_agg = df_telemetry.groupBy(\"pipeline_segment_id\") \\\n",
    "    .agg(\n",
    "        avg(\"pressure_psi\").alias(\"avg_pressure\"),\n",
    "        avg(\"temperature_c\").alias(\"avg_temp\"),\n",
    "        max(\"vibration_level\").alias(\"max_vibration\")\n",
    "    )\n",
    "\n",
    "# Join with Inspections\n",
    "df_final = df_inspections.join(df_telemetry_agg, \"pipeline_segment_id\", \"left\")\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../data/processed_pipeline_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# 4. Persist to Parquet\n",
    "output_path = \"../data/processed_pipeline_data.parquet\"\n",
    "df_final.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
